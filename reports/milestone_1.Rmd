---
title: "A Quick Corpus Study - Milestone Report"
author: "J.E. Turcotte"
date: "September 4, 2016"
output: html_document
---

```{r loading and labelling the data, message=F, warning=F, echo=F }
library(ggplot2)
library(grid)
library(gridExtra)
library(knitr)
library(pander)
library(car)
library(caret)
library(reshape2)

set.seed(7041)

grid_arrange_shared_legend <- function(...) {
    plots <- list(...)
    g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
    legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    grid.arrange(
        do.call(arrangeGrob, lapply(plots, function(x)
            x + theme(legend.position="none"))),
        legend,
        ncol = 1,
        heights = unit.c(unit(1, "npc") - lheight, lheight)
    )
}
```
```{r get the data, echo=F, warning=F}

bigrams <- read.csv(
     "data/evaluated/sample_bigrams.csv",
     stringsAsFactors = FALSE
)

trigrams <- read.csv(
     "data/evaluated/sample_trigrams.csv",
     stringsAsFactors = FALSE
)

blogs_words <- read.csv(
     "data/evaluated/blogs_words.csv",
     stringsAsFactors = FALSE
)

news_words <- read.csv(
     "data/evaluated/news_words.csv",
     stringsAsFactors = FALSE
)

twitter_words <- read.csv(
     "data/evaluated/twitter_words.csv",
     stringsAsFactors = FALSE
)

blogs_stats <- read.csv(
     "data/evaluated/blogs_stats.csv",
     stringsAsFactors = FALSE
)

news_stats <- read.csv(
     "data/evaluated/news_stats.csv",
     stringsAsFactors = FALSE
)

twitter_stats <- read.csv(
     "data/evaluated/twitter_stats.csv",
     stringsAsFactors = FALSE
)

ngram_stats <- read.csv(
     "data/evaluated/bigram_and_trigram_stats.csv",
     stringsAsFactors = FALSE
)

```

Herein is a simplie report of progress being made toward and end goal of developing a predictive model useful in guessing a typer's next most probable word, based on a large corpus of blog posts, news articles, and tweets.  At this early stage, a truly niave method is utilized in analyzing the contents of these files.

## Processing

Each file was re-processed, cutting each provided line into any number of sentences, to be studied as cohesive thoughts.  The results were then cleaned of any further punctuation, capital letters and the like, and cached as such for later tallies.  These then were studied for word counts versus unique word counts (effectively unigrams) as well as bi- and tri-gram groupings, again for total population and counts for the unique among them.  From news and blog entries, we pulled 50,000 records and random, and (given the brevity of tweets at 140 charcters, maximum) 250,000 from twitter.  The results were, again, cached to .csv file for ease of consideration at the time of this report.

## Basic Details about Each Corpus

```{r whittle down the words a bit, message=F, warning=F, echo=F }
blogs_words <- blogs_words[blogs_words$word != '',]
blogs_words <- blogs_words[blogs_words$V1 > 1,]
news_words <- news_words[news_words$word != '',]
news_words <- news_words[news_words$V1 > 1,]
twitter_words <- twitter_words[twitter_words$word != '',]
twitter_words <- twitter_words[twitter_words$V1 > 1,]
```
```{r assembling some data, message=F, warning=F, echo=F }
raw_stats <- rbind( blogs_stats, news_stats, twitter_stats )
rownames(raw_stats) <- c('Blogs','News','Twitter')
raw_stats$most_common <- c('','','')
raw_stats['Blogs',]$most_common <- paste( head( blogs_words[order( blogs_words$V1, decreasing=T ),], n=4 )$word, collapse=", " )
raw_stats['News',]$most_common <- paste( head( news_words[order( news_words$V1, decreasing=T ),], n=4 )$word, collapse=", " )
raw_stats['Twitter',]$most_common <- paste( head( twitter_words[order( twitter_words$V1, decreasing=T ),], n=4 )$word, collapse=", " )
colnames(raw_stats) <- c('Studied Entries','Total Words','Unique Words','Most Common')
pander(raw_stats)
```
As must be no surprise, basic words such as 'the', 'and', and 'to' dominate the word counts.  They do vary in order, however, suggesting different styles of writing technique, as further exemplified by the the use of 'i' in the twitter feed.  Given that blogs might be akin to a halfway point between tweets (which usually are self-referential) and news (which significantly more complex architecture), let's pick and compare some words common to each from this collection... albeit, a few common but not extremely common words, and a few uncommon as well.

# Some Findings about Word Use
```{r use blogs to compare to news and tweets, message=F, warning=F, echo=F, fig.width=10, fig.height=3 }

# take a subset of the blog corpus, winnowing things down to common but not extremely
# common words AND some uncommon but not rare ones
subset_of_blogs <- blogs_words[ blogs_words$V1 <= 10000 & blogs_words$V1 >= 1200, ]
subset_of_blogs <- subset_of_blogs[ order( subset_of_blogs$V1, decreasing=T ), ]
chosen_words <- data.frame( word=character(), group=character(), count=integer() )
chosen <- c( head( subset_of_blogs$word, n=10 ), tail( subset_of_blogs$word, n=10 ) )

# now get those counts into a new data structure
comparing_the_chosen <- blogs_words[blogs_words$word %in% chosen,c(2,1)]
comparing_the_chosen[,3] <- news_words[news_words$word %in% chosen,1]
comparing_the_chosen[,4] <- twitter_words[twitter_words$word %in% chosen,1]
comparing_the_chosen <- comparing_the_chosen[ order(comparing_the_chosen$V1, decreasing=T ), ]

# now turn them into ratios, since our word count from each do not match
colnames(comparing_the_chosen) <- c('Word','Blogs','News','Twitter')
comparing_the_chosen$Blogs <- ( comparing_the_chosen$Blogs / sum(blogs_words$V1) ) * 100
comparing_the_chosen$News <- ( comparing_the_chosen$News / sum(news_words$V1) ) * 100
comparing_the_chosen$Twitter <- ( comparing_the_chosen$Twitter / sum(twitter_words$V1) ) * 100

# and now make this displayable
comparing_the_chosen <- melt( comparing_the_chosen )
colnames(comparing_the_chosen) <- c('Word','Source','Percent')
wp <- ggplot( comparing_the_chosen, aes( reorder(Word,-Percent), Percent, fill=Source) )
wp <- wp + geom_bar( stat="identity", position='dodge' ) #aes(fill=Source)
wp + xlab("10 Common and 10 Uncommon Words") + ylab("Percent of All Words Used")
```
Between blogs and news, shorter and more commonly used words are used in relatively similar proportion throughout the sample sets, though some seem to be more often sacrificed for brevity (presumably) within tweets.  Not too surprisingly, blogs seem to mention 'me' vastly more often than the other two combined.  Further, less common words appear to be more commonly used on twitter than elsewhere... again, perhaps there's not enough room for the most common in a tweet.

## Further Work

Going forward, in trying to develop a predictive model, thus, it may be important to either cull the connective words like these, as a twitter user naturally seems to be doing.

Efforts to collate bi-grams and tri-grams are ongoing, but proving very time consuming, and practices for making this more efficient will almost certainly be required in the next week, for the next report.